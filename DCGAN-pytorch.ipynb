{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"bsize\" : 128,# Batch size during training.\n",
    "    'imsize' : 64,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 3,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 64,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 64, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 1,# Number of training epochs.\n",
    "    'lr' : 0.0002,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 2}# Save step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset link\n",
    "# https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"data/\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(params['imsize']),\n",
    "    transforms.CenterCrop(params['imsize']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "device = torch.device(\"cudo:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "dataset = dset.ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=params['bsize'], shuffle=True)\n",
    "\n",
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis('off')\n",
    "plt.title('Training Images')\n",
    "plt.imshow(np.transpose(vutils.make_grid(sample_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.normal_(w.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Input is the latent vector Z.\n",
    "        self.tconv1 = nn.ConvTranspose2d(params['nz'], params['ngf']*8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(params['ngf']*8)\n",
    "\n",
    "        # Input Dimension: (ngf*8) x 4 x 4\n",
    "        self.tconv2 = nn.ConvTranspose2d(params['ngf']*8, params['ngf']*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ngf']*4)\n",
    "\n",
    "        # Input Dimension: (ngf*4) x 8 x 8\n",
    "        self.tconv3 = nn.ConvTranspose2d(params['ngf']*4, params['ngf']*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ngf']*2)\n",
    "\n",
    "        # Input Dimension: (ngf*2) x 16 x 16\n",
    "        self.tconv4 = nn.ConvTranspose2d(params['ngf']*2, params['ngf'], kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ngf'])\n",
    "\n",
    "        # Input Dimension: (ngf) * 32 * 32\n",
    "        self.tconv5 = nn.ConvTranspose2d(params['ngf'], params['nc'], kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        #Output Dimension: (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.tconv1(x)))\n",
    "        x = F.relu(self.bn2(self.tconv2(x)))\n",
    "        x = F.relu(self.bn3(self.tconv3(x)))\n",
    "        x = F.relu(self.bn4(self.tconv4(x)))\n",
    "\n",
    "        x = F.tanh(self.tconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Input: nc*64*64\n",
    "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'], kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        # Input: ndf*32*32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "        \n",
    "        # Input: (ndf*2)*16*16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "        \n",
    "        # Input: (ndf*4)*8*8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "        \n",
    "        # Input: (ndf*8)*4*4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 1, kernel_size=4, stride=2, padding=0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generator :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (tconv1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tconv5): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(params)\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dicriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator(params)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas = (params['beta1'], 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas = (params['beta1'], 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# stores generated images as training progresses\n",
    "img_lis = []\n",
    "\n",
    "#stores discriminator losses during training\n",
    "D_losses = []\n",
    "\n",
    "#stores generator loss during training\n",
    "G_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop: ... \n",
      "****************************************************************************************************\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(0/1583)\t loss_D: 0.4348\t loss_G: 4.5222\t D(x): 0.8832\tD(G(z)): 0.2491/0.0116\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(50/1583)\t loss_D: 0.1726\t loss_G: 6.3919\t D(x): 0.8911\tD(G(z)): 0.0021/0.0024\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(100/1583)\t loss_D: 0.0925\t loss_G: 6.3017\t D(x): 0.9499\tD(G(z)): 0.0332/0.0025\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(150/1583)\t loss_D: 0.4693\t loss_G: 3.1881\t D(x): 0.7775\tD(G(z)): 0.1691/0.0517\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(200/1583)\t loss_D: 0.5013\t loss_G: 3.2970\t D(x): 0.8206\tD(G(z)): 0.2382/0.0427\n",
      "Is cuda avaialable:  False\n",
      "(0/1)(250/1583)\t loss_D: 0.3916\t loss_G: 3.5455\t D(x): 0.8979\tD(G(z)): 0.2355/0.0370\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-cef16dacf689>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mzero\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0miteration\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0merrG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mD_G_Z2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaudel\\my_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaudel\\my_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters = 0 \n",
    "print(\"Starting training loop: ... \")\n",
    "print(\"*\"*130)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader,0):\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        # Make accumulated gradient of the disciminator zero\n",
    "        netD.zero_grad()\n",
    "        # create label for real data (label=1)\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        output = netD(real_data).view(-1)\n",
    "        errorD_real = criterion(output,label)\n",
    "        \n",
    "        # Calculate gradient for backpropagation\n",
    "        errorD_real.backward()\n",
    "        D_X = output.mean().item()\n",
    "        \n",
    "        # Sample random data from a unit normal distribution\n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        \n",
    "        # generate fake images\n",
    "        fake_data = netG(noise)\n",
    "        \n",
    "        #create label for fake data (label=0)\n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the output of the discriminator of the fake data.\n",
    "        As no gradients w.r.t. the generator parameters are to be\n",
    "        calculated, detach() is used. Hence, only gradients w.r.t. the\n",
    "        discriminator parameters will be calculated.\n",
    "        This is done because the loss functions for the discriminator\n",
    "        and the generator are slightly different.\n",
    "        \"\"\"\n",
    "        \n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errorD_fake = criterion(output, label)\n",
    "        errorD_fake.backward()\n",
    "        D_G_Z1 = output.mean().item()\n",
    "        \n",
    "        # Net discriminator loss\n",
    "        errD = errorD_real + errorD_fake\n",
    "        \n",
    "        #update discriminator parameters\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Make accumalted gradients of the generator zero.\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # we want the fake data to be classified as real. Hence real labels are used (label=1)\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "        \"\"\"\n",
    "        gradients for backpropagation are calculated. Gradients w.r.t both the generator\n",
    "        and discriminator parameters are calculated. However, the generators optimizer will\n",
    "        only update the parameters of the generator. The discriminator gradients will be \n",
    "        set to zero in the next iteration by netD.zero_grad()\n",
    "        \"\"\"\n",
    "        errG.backward()\n",
    "        \n",
    "        D_G_Z2 = output.mean().item()\n",
    "        \n",
    "        # update the generator parameters\n",
    "        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Check the progress of training \n",
    "        \n",
    "        if i%50 == 0:\n",
    "            print(\"Is cuda avaialable: \", torch.cuda.is_available())\n",
    "            print('(%d/%d)(%d/%d)\\t loss_D: %.4f\\t loss_G: %.4f\\t D(x): %.4f\\tD(G(z)): %.4f/%.4f' \n",
    "                  %(epoch, params['nepochs'], i, len(dataloader),errD.item(), errG.item(), D_X, D_G_Z1, D_G_Z2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
